model_param:
  mode: "LLM"
  run_type: "training"
  tied_embeddings: true
  model_type: "llama"  # gpt assumes dense-style architecture with GELU; llama assumes llama-style architecture with SwiGLU
  batch_size: 1024
  seq_len: 2048
  decode_len: 1024
  hidden_dim: 16384
  attention:
    attention_type: "gqa"  # one of "mha","gqa","mla"
    num_heads: 128
    kv_heads: 8
    use_flashattention: false  # Whether to use flash attention
    attention_tile_size: null  # Tile size for attention computation
  MOE:
    use_moe: false  # Whether to use Mixture of Experts layers
    num_experts: null  # Number of experts in MoE layers
    top_k: null  # Top-k experts to use in MoE layers
  ffn_dim: 53248
  ffn_mult: null
  vocab_size: 128256
  num_layers: 126
inference_param:
  sample_every: -1
